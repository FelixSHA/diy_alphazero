{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Aj65YIOgpjl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.trajectories import time_step as ts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UO7jW2Axhli0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConnectFourEnv(py_environment.PyEnvironment):\n",
        "    def __init__(self):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=6, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(6, 7), dtype=np.int32, minimum=0, maximum=2, name='observation')\n",
        "        self._state = np.zeros((6, 7), dtype=np.int32)\n",
        "        self._current_player = 1\n",
        "        self._episode_ended = False\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self._state = np.zeros((6, 7), dtype=np.int32)\n",
        "        self._current_player = 1\n",
        "        self._episode_ended = False\n",
        "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
        "\n",
        "    def _step(self, action):\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        if np.all(self._state[:, action] != 0):\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(np.array(self._state, dtype=np.int32), -10)\n",
        "\n",
        "        row = np.max(np.where(self._state[:, action] == 0))\n",
        "        self._state[row, action] = self._current_player\n",
        "\n",
        "        if self._check_win(self._current_player):\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(np.array(self._state, dtype=np.int32), 10)\n",
        "\n",
        "        if np.all(self._state != 0):\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(np.array(self._state, dtype=np.int32), 0)\n",
        "\n",
        "        self._current_player = 1 if self._current_player == 2 else 2\n",
        "        return ts.transition(np.array(self._state, dtype=np.int32), reward=0.0, discount=1.0)\n",
        "\n",
        "    def _check_win(self, player):\n",
        "      # Horizontal check\n",
        "      for c in range(7-3):\n",
        "          for r in range(6):\n",
        "              if self._state[r][c] == player and self._state[r][c+1] == player and self._state[r][c+2] == player and self._state[r][c+3] == player:\n",
        "                  return True\n",
        "\n",
        "      # Vertical check\n",
        "      for c in range(7):\n",
        "          for r in range(6-3):\n",
        "              if self._state[r][c] == player and self._state[r+1][c] == player and self._state[r+2][c] == player and self._state[r+3][c] == player:\n",
        "                  return True\n",
        "\n",
        "      # Positive diagonal check\n",
        "      for c in range(7-3):\n",
        "          for r in range(6-3):\n",
        "              if self._state[r][c] == player and self._state[r+1][c+1] == player and self._state[r+2][c+2] == player and self._state[r+3][c+3] == player:\n",
        "                  return True\n",
        "\n",
        "      # Negative diagonal check\n",
        "      for c in range(7-3):\n",
        "          for r in range(3, 6):\n",
        "              if self._state[r][c] == player and self._state[r-1][c+1] == player and self._state[r-2][c+2] == player and self._state[r-3][c+3] == player:\n",
        "                  return True\n",
        "\n",
        "      return False\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4KOcUL43htAm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the Python environment to a TensorFlow environment.\n",
        "train_env = tf_py_environment.TFPyEnvironment(ConnectFourEnv())\n",
        "eval_env = tf_py_environment.TFPyEnvironment(ConnectFourEnv())\n",
        "\n",
        "# Initialize the QNetwork.\n",
        "fc_layer_params = (100,)\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "# Initialize the agent.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "train_step_counter = tf.Variable(0)\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "agent.initialize()\n",
        "\n",
        "# Initialize the replay buffer.\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=100000)\n",
        "\n",
        "# Initialize the data collection policy and collect some initial data.\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "initial_collect_steps = 1000\n",
        "for _ in range(initial_collect_steps):\n",
        "    time_step = train_env.current_time_step()\n",
        "    action_step = random_policy.action(time_step)\n",
        "    next_time_step = train_env.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "    replay_buffer.add_batch(traj)\n",
        "\n",
        "# Set up the dataset.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Hh2RHkh1Ou",
        "outputId": "e0794f0a-567b-416f-b8d1-25ca6dc85dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 200: loss = 15.778207778930664\n",
            "step = 400: loss = 44.207244873046875\n",
            "step = 600: loss = 53.20875549316406\n",
            "step = 800: loss = 31.317123413085938\n",
            "step = 1000: loss = 8.032379150390625\n",
            "step = 1200: loss = 7.57925271987915\n",
            "step = 1400: loss = 1.5052998065948486\n",
            "step = 1600: loss = 10.81866455078125\n",
            "step = 1800: loss = 2.0842370986938477\n",
            "step = 2000: loss = 17.604158401489258\n",
            "step = 2200: loss = 2.1102828979492188\n",
            "step = 2400: loss = 6.711139678955078\n",
            "step = 2600: loss = 3.1410837173461914\n",
            "step = 2800: loss = 0.8111436367034912\n",
            "step = 3000: loss = 2.0690407752990723\n",
            "step = 3200: loss = 2.0183353424072266\n",
            "step = 3400: loss = 2.814595937728882\n",
            "step = 3600: loss = 0.9827808141708374\n",
            "step = 3800: loss = 5.350802421569824\n",
            "step = 4000: loss = 3.635037899017334\n",
            "step = 4200: loss = 1.4304511547088623\n",
            "step = 4400: loss = 1.0700972080230713\n",
            "step = 4600: loss = 3.3593015670776367\n",
            "step = 4800: loss = 0.6524667739868164\n",
            "step = 5000: loss = 1.956712007522583\n",
            "step = 5200: loss = 2.027306318283081\n",
            "step = 5400: loss = 0.9282756447792053\n",
            "step = 5600: loss = 0.40947961807250977\n",
            "step = 5800: loss = 6.800469398498535\n",
            "step = 6000: loss = 0.6997165083885193\n",
            "step = 6200: loss = 0.46664050221443176\n",
            "step = 6400: loss = 2.062587261199951\n",
            "step = 6600: loss = 1.330338954925537\n",
            "step = 6800: loss = 0.6640044450759888\n",
            "step = 7000: loss = 0.3101568818092346\n",
            "step = 7200: loss = 0.7275099158287048\n",
            "step = 7400: loss = 0.593917191028595\n",
            "step = 7600: loss = 0.8546889424324036\n",
            "step = 7800: loss = 0.5946536064147949\n",
            "step = 8000: loss = 0.5862564444541931\n",
            "step = 8200: loss = 2.0014796257019043\n",
            "step = 8400: loss = 1.4627947807312012\n",
            "step = 8600: loss = 0.5618829727172852\n",
            "step = 8800: loss = 2.2537829875946045\n",
            "step = 9000: loss = 0.5437129735946655\n",
            "step = 9200: loss = 0.6822471618652344\n",
            "step = 9400: loss = 0.40480154752731323\n",
            "step = 9600: loss = 5.248859882354736\n",
            "step = 9800: loss = 3.9010703563690186\n",
            "step = 10000: loss = 3.4812605381011963\n",
            "step = 10200: loss = 0.7477918863296509\n",
            "step = 10400: loss = 2.8463141918182373\n",
            "step = 10600: loss = 0.8296219110488892\n",
            "step = 10800: loss = 2.8825294971466064\n",
            "step = 11000: loss = 1.7363371849060059\n",
            "step = 11200: loss = 0.6778403520584106\n",
            "step = 11400: loss = 2.828338623046875\n",
            "step = 11600: loss = 0.9682790040969849\n",
            "step = 11800: loss = 1.1822644472122192\n",
            "step = 12000: loss = 2.8063974380493164\n",
            "step = 12200: loss = 1.5980432033538818\n",
            "step = 12400: loss = 2.853522777557373\n",
            "step = 12600: loss = 0.40571337938308716\n",
            "step = 12800: loss = 0.9414839148521423\n",
            "step = 13000: loss = 0.551963210105896\n",
            "step = 13200: loss = 1.8782317638397217\n",
            "step = 13400: loss = 0.9312947392463684\n",
            "step = 13600: loss = 0.5434659719467163\n",
            "step = 13800: loss = 0.6558734178543091\n",
            "step = 14000: loss = 1.327124834060669\n",
            "step = 14200: loss = 1.72621488571167\n",
            "step = 14400: loss = 2.3533685207366943\n",
            "step = 14600: loss = 0.7437056303024292\n",
            "step = 14800: loss = 1.8739190101623535\n",
            "step = 15000: loss = 3.867645025253296\n",
            "step = 15200: loss = 1.323270320892334\n",
            "step = 15400: loss = 4.8417158126831055\n",
            "step = 15600: loss = 2.0799129009246826\n",
            "step = 15800: loss = 1.6173557043075562\n",
            "step = 16000: loss = 1.2375264167785645\n",
            "step = 16200: loss = 0.9153125286102295\n",
            "step = 16400: loss = 3.4584598541259766\n",
            "step = 16600: loss = 1.1497864723205566\n",
            "step = 16800: loss = 1.1599574089050293\n",
            "step = 17000: loss = 1.2736797332763672\n",
            "step = 17200: loss = 2.3306853771209717\n",
            "step = 17400: loss = 0.527800440788269\n",
            "step = 17600: loss = 1.5940706729888916\n",
            "step = 17800: loss = 1.3278542757034302\n",
            "step = 18000: loss = 1.3018798828125\n",
            "step = 18200: loss = 0.9404615163803101\n",
            "step = 18400: loss = 7.649378776550293\n",
            "step = 18600: loss = 0.8430314064025879\n",
            "step = 18800: loss = 0.681050181388855\n",
            "step = 19000: loss = 2.1130294799804688\n",
            "step = 19200: loss = 2.7130672931671143\n",
            "step = 19400: loss = 1.1830400228500366\n",
            "step = 19600: loss = 0.9950369596481323\n",
            "step = 19800: loss = 0.6566449999809265\n",
            "step = 20000: loss = 0.7300510406494141\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the agent.\n",
        "num_iterations = 20000\n",
        "log_interval = 200\n",
        "\n",
        "@tf.function\n",
        "def train_step():\n",
        "    time_step = train_env.current_time_step()\n",
        "    action_step = agent.collect_policy.action(time_step)\n",
        "    next_time_step = train_env.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "    replay_buffer.add_batch(traj)\n",
        "\n",
        "    # Opponent's turn\n",
        "    if not next_time_step.is_last():\n",
        "        time_step = train_env.current_time_step()\n",
        "        action_step = random_policy.action(time_step)\n",
        "        next_time_step = train_env.step(action_step.action)\n",
        "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "        replay_buffer.add_batch(traj)\n",
        "\n",
        "    experience, unused_info = next(iterator)\n",
        "    return agent.train(experience).loss\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    train_loss = train_step()\n",
        "    if train_step_counter.numpy() % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(train_step_counter.numpy(), train_loss))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
