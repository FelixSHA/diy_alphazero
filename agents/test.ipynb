{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from random import sample, choice\n",
    "from math import exp, log\n",
    "import random\n",
    "from random import choice\n",
    "from math import exp, log\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        # Initialize game parameters\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        # Return an empty grid representing the initial state of the game\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        # Update the state by placing the player's token in the specified column\n",
    "        row = np.max(np.where(state[:, action] == 0))  # Find the lowest empty row in the selected column\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        if len(state.shape) == 3:\n",
    "            # If the input is a batch of states, return valid moves for each state in the batch\n",
    "            return (state[:, 0] == 0).astype(np.uint8)\n",
    "        else:\n",
    "            # Return a binary array indicating valid moves (columns) where tokens can be placed\n",
    "            return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        # Get the row, column, and player corresponding to the last action\n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            # Helper function to count the number of consecutive tokens in a given direction\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = column + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        # Check for win conditions in vertical, horizontal, and diagonal directions\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1  # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1  # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1  # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1  # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        # Check if the last action resulted in a win for the player\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        # Check if the game ended in a draw\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        # If the game is not terminated, return a value of 0\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        # Return the opponent of a given player\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        # Return the opponent's value given the player's value\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        # Change the perspective of the state to the given player\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        # Encode the state in a suitable format for training a machine learning model\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state\n",
    "    \n",
    "# class TicTacToe:\n",
    "#     def __init__(self):\n",
    "#         self.row_count = 3\n",
    "#         self.column_count = 3\n",
    "#         self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return \"TicTacToe\"\n",
    "        \n",
    "#     def get_initial_state(self):\n",
    "#         return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "#     def get_next_state(self, state, action, player):\n",
    "#         row = action // self.column_count\n",
    "#         column = action % self.column_count\n",
    "#         state[row, column] = player\n",
    "#         return state\n",
    "    \n",
    "#     def get_valid_moves(self, state):\n",
    "#         if len(state.shape) == 3:\n",
    "#             return (state.reshape(-1, 9) == 0).astype(np.uint8)\n",
    "#         return (state.reshape(9) == 0).astype(np.uint8)\n",
    "    \n",
    "#     def check_win(self, state, action):\n",
    "#         if action == None:\n",
    "#             return False\n",
    "        \n",
    "#         row = action // self.column_count\n",
    "#         column = action % self.column_count\n",
    "#         player = state[row, column]\n",
    "        \n",
    "#         return (\n",
    "#             np.sum(state[row, :]) == player * self.column_count\n",
    "#             or np.sum(state[:, column]) == player * self.row_count\n",
    "#             or np.sum(np.diag(state)) == player * self.row_count\n",
    "#             or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "#         )\n",
    "    \n",
    "#     def get_value_and_terminated(self, state, action):\n",
    "#         if self.check_win(state, action):\n",
    "#             return 1, True\n",
    "#         if np.sum(self.get_valid_moves(state)) == 0:\n",
    "#             return 0, True\n",
    "#         return 0, False\n",
    "    \n",
    "#     def get_opponent(self, player):\n",
    "#         return -player\n",
    "    \n",
    "#     def get_opponent_value(self, value):\n",
    "#         return -value\n",
    "    \n",
    "#     def change_perspective(self, state, player):\n",
    "#         return state * player\n",
    "    \n",
    "#     def get_encoded_state(self, state):\n",
    "#         encoded_state = np.stack(\n",
    "#             (state == -1, state == 0, state == 1)\n",
    "#         ).astype(np.float32)\n",
    "        \n",
    "#         if len(state.shape) == 3:\n",
    "#             encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "#         return encoded_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "      def __init__(self, game, args):\n",
    "            self.game = game\n",
    "            self.args = args\n",
    "\n",
    "      def selfPlay(self):\n",
    "            np.random.seed(self.args['random_state'])\n",
    "            memory = []\n",
    "            player = 1\n",
    "            state = self.game.get_initial_state()\n",
    "\n",
    "            while True:\n",
    "                  neutral_state = self.game.change_perspective(state, player)\n",
    "\n",
    "                  memory.append((neutral_state, None, player))\n",
    "                  action = np.random.choice(self.game.action_size, )\n",
    "\n",
    "                  state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "                  time.sleep(0.5)\n",
    "                  print(\n",
    "                        f\"Player {player} played action {action} and the state is now:\"\n",
    "                  )\n",
    "                  print(state)\n",
    "                  print()\n",
    "\n",
    "                  value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "                  if is_terminal:\n",
    "                        returnMemory = []\n",
    "                        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                              hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                              returnMemory.append((\n",
    "                                    self.game.get_encoded_state(hist_neutral_state),\n",
    "                                    hist_action_probs,\n",
    "                                    hist_outcome\n",
    "                              ))\n",
    "                        return returnMemory\n",
    "\n",
    "                  player = self.game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from games import ConnectFour\n",
    "# from agents.RandomPlayer import RandomPlayer\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#       game = ConnectFour()\n",
    "#       args = {\n",
    "#             'random_state': 42\n",
    "#             }\n",
    "#       player = RandomPlayer(game, args)\n",
    "#       player.selfPlay()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepQ Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, state_size, action_size):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_size, 20)\n",
    "#         self.fc2 = nn.Linear(20, 50)\n",
    "#         self.fc3 = nn.Linear(50, action_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQNAgent:\n",
    "#     def __init__(self, state_size, action_size, episodes):\n",
    "#         self.state_size = state_size\n",
    "#         self.action_size = action_size\n",
    "#         self.memory = deque(maxlen=500)\n",
    "#         self.gamma = 0.9   # discount rate\n",
    "#         self.epsilon = 0.10  # initial exploration rate\n",
    "#         self.epsilon_min = 0.01\n",
    "#         self.epsilon_decay = exp((log(self.epsilon_min) - log(self.epsilon))/(0.8*episodes)) # reaches epsilon_min after 80% of iterations\n",
    "#         self.model = DQN(self.state_size, self.action_size)\n",
    "#         self.criterion = nn.MSELoss()\n",
    "#         self.optimizer = optim.Adam(self.model.parameters(), lr=0.00001)\n",
    "    \n",
    "#     def memorize(self, state, action, reward, next_state, done):\n",
    "#         self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "#     def act(self, state):\n",
    "#         if np.random.rand() <= self.epsilon: # Exploration\n",
    "#             return choice([c for c in range(self.action_size) if state[:,c] == 0])\n",
    "#         state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "#         with torch.no_grad():\n",
    "#             act_values = self.model(state)\n",
    "#         action = torch.argmax(act_values).item()\n",
    "#         return action\n",
    "    \n",
    "#     def replay(self, batch_size):\n",
    "#         minibatch = random.sample(self.memory, batch_size)\n",
    "#         for state, action, reward, next_state, done in minibatch:\n",
    "#             state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "#             next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "#             reward = torch.tensor([reward]).float()\n",
    "#             action = torch.tensor([action])\n",
    "#             done = torch.tensor([done]).float()\n",
    "\n",
    "#             if not done:\n",
    "#                 target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "#             else:\n",
    "#                 target = reward\n",
    "#             current = self.model(state)[0][action].item()\n",
    "#             loss = self.criterion(target, current)\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "            \n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "#     def load(self, name):\n",
    "#         self.model.load_state_dict(torch.load(name))\n",
    "    \n",
    "#     def save(self, name):\n",
    "#         torch.save(self.model.state_dict(), name)\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, episodes, environment):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500)\n",
    "        self.gamma = 0.9   # discount rate\n",
    "        self.epsilon = 0.10  # initial exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = exp((log(self.epsilon_min) - log(self.epsilon))/(0.8*episodes)) # reaches epsilon_min after 80% of iterations\n",
    "        self.model = self._build_model()\n",
    "        self.env = environment  # ConnectFour or TicTacToe environment\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr = 0.00001))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        valid_moves = self.env.get_valid_moves(state)  # Get valid moves for current state\n",
    "        if np.random.rand() <= self.epsilon: # Exploration\n",
    "            return choice([c for c in np.where(valid_moves == 1)[0]])\n",
    "        act_values = self.model.predict(state) # Exploitation\n",
    "        # Remove invalid actions by setting their value to negative infinity\n",
    "        act_values[0][np.where(valid_moves == 0)[0]] = -np.inf\n",
    "        action = np.argmax(act_values[0])\n",
    "        return action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 126), found shape=(None, 3, 6, 7)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     70\u001b[0m     \u001b[39m# Decide action\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(state)\n\u001b[0;32m     72\u001b[0m     next_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_next_state(np\u001b[39m.\u001b[39mcopy(state), action, player)\n\u001b[0;32m     73\u001b[0m     reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_value_and_terminated(next_state, action)\n",
      "Cell \u001b[1;32mIn[31], line 85\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon: \u001b[39m# Exploration\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m choice([c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mwhere(valid_moves \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]])\n\u001b[1;32m---> 85\u001b[0m act_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state) \u001b[39m# Exploitation\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39m# Remove invalid actions by setting their value to negative infinity\u001b[39;00m\n\u001b[0;32m     87\u001b[0m act_values[\u001b[39m0\u001b[39m][np\u001b[39m.\u001b[39mwhere(valid_moves \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n",
      "File \u001b[1;32mc:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileucs8i8ve.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Felix\\.virtualenvs\\diy_alphazero-3BngPyJJ\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 126), found shape=(None, 3, 6, 7)\n"
     ]
    }
   ],
   "source": [
    "# # initialize gym environment and the agent\n",
    "# env = ConnectFour()\n",
    "# state_size = env.row_count * env.column_count * 3\n",
    "# action_size = env.action_size\n",
    "# episodes = 40000\n",
    "# agent = DQNAgent(state_size, action_size, episodes)\n",
    "# # agent.load(\"./connectX-weights_deep.pth\") # commented out\n",
    "# batch_size = 40\n",
    "\n",
    "# # Monitoring devices\n",
    "# all_total_rewards = np.empty(episodes)\n",
    "# all_avg_rewards = np.empty(episodes)\n",
    "\n",
    "# # Iterate the game\n",
    "# for e in range(episodes):\n",
    "#     # reset state in the beginning of each game\n",
    "#     done = False\n",
    "#     state = env.get_initial_state()\n",
    "#     total_rewards = 0\n",
    "#     while not done:\n",
    "#         # Decide action\n",
    "#         action = int(agent.act(state))\n",
    "#         next_state = env.get_next_state(state, action, 1)\n",
    "#         value, done = env.get_value_and_terminated(next_state, action)\n",
    "#         reward = value if done else 0.0\n",
    "        \n",
    "#         # invalid move: hard penalization\n",
    "#         if env.get_valid_moves(state)[action] == 0:\n",
    "#             reward = -10\n",
    "#         agent.memorize(state, action, reward, next_state, done)\n",
    "\n",
    "#         # make next_state the new current state for the next frame.\n",
    "#         state = next_state\n",
    "#         total_rewards += reward\n",
    "\n",
    "#     # experience replay\n",
    "#     if len(agent.memory) > batch_size:\n",
    "#         agent.replay(batch_size)\n",
    "        \n",
    "#     all_total_rewards[e] = total_rewards\n",
    "#     avg_reward = all_total_rewards[max(0, e - 100):e].mean()\n",
    "#     all_avg_rewards[e] = avg_reward\n",
    "#     if e % 100 == 0 :\n",
    "#         agent.save(\"./connectX-weights_deep.pth\")\n",
    "#         print(\"episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\".format(e, episodes, agent.epsilon, avg_reward))\n",
    "\n",
    "# initialize gym environment and the agent\n",
    "env = ConnectFour()\n",
    "state_size = env.row_count * env.column_count * 3  # state representation is 3 channels of the board\n",
    "action_size = env.column_count\n",
    "episodes = 40000\n",
    "agent = DQNAgent(state_size, action_size, episodes, env)\n",
    "# agent.load(\"./connectX-weights_deep.h5\") # load prelearned weights\n",
    "batch_size = 40 # Don't know if this number makes sense\n",
    "\n",
    "# Monitoring devices\n",
    "all_total_rewards = np.empty(episodes)\n",
    "all_avg_rewards = np.empty(episodes)\n",
    "\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.get_initial_state()\n",
    "    state = env.get_encoded_state(state)\n",
    "    state = np.expand_dims(state, axis=0)  # add a dimension for batch compatibility\n",
    "    total_rewards = 0\n",
    "    player = 1\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "        next_state = env.get_next_state(np.copy(state), action, player)\n",
    "        reward, done = env.get_value_and_terminated(next_state, action)\n",
    "        if env.get_valid_moves(state)[action] == 0:  # invalid move\n",
    "            reward = -10\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        player = env.get_opponent(player)  # switch player\n",
    "        total_rewards += reward\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        all_total_rewards[e] = total_rewards\n",
    "        avg_reward = all_total_rewards[max(0, e - 100):e].mean()\n",
    "        all_avg_rewards[e] = avg_reward\n",
    "        if e % 100 == 0 :\n",
    "            agent.save(\"./connectX-weights_deep.h5\")\n",
    "            print(\"episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\".format(e, episodes, agent.epsilon, avg_reward))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy_alphazero-3BngPyJJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
